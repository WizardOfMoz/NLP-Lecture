{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "PpcoqA-3l2g9",
      "metadata": {
        "id": "PpcoqA-3l2g9"
      },
      "source": [
        "## Installing Sentence Transsformer and other models/frameworks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "id": "0tw0xe31hT59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tw0xe31hT59",
        "outputId": "1dcff28e-580e-49de-c5f3-89187828229d"
      },
      "outputs": [],
      "source": [
        "# !pip3 install sentence_transformers\n",
        "# !pip3 install gensim\n",
        "# Kindly add all your installations and versions if any in this cell."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WWWTQtXDl-t8",
      "metadata": {
        "id": "WWWTQtXDl-t8"
      },
      "source": [
        "## Importing necessary libraries. \n",
        "In the final version all imports should be stricly enlisted here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "id": "e951f9be",
      "metadata": {
        "id": "e951f9be"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "import spacy\n",
        "from scipy import stats\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "import gensim.downloader\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, losses, models, util\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from sentence_transformers.readers import InputExample\n",
        "\n",
        "import torch \n",
        "from torch.utils.data import DataLoader\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H5Ch9I58mMGe",
      "metadata": {
        "id": "H5Ch9I58mMGe"
      },
      "source": [
        "## Load dataset: 7 marks\n",
        "1 Download and unzip the dataset from this link http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz  **1 mark**\n",
        "\n",
        "2 Complete the code in `read_sts_csv()`. **4.5 marks**\n",
        "\n",
        "3 Create 3 dataframes one each for train, test and val and print their final shapes. **1.5 marks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "id": "694f1fab",
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(' +', ' ',sentence) #remove extra whitespaces\n",
        "    sentence =re.sub('[^\\w\\s]','',sentence) #keep only words and spaces\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "id": "2TMR0Z0DlfFf",
      "metadata": {
        "id": "2TMR0Z0DlfFf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content/sts-train.csv\n",
            "content/sts-dev.csv\n",
            "content/sts-test.csv\n"
          ]
        }
      ],
      "source": [
        "INPUT_PATH='content/'\n",
        "def read_sts_csv(dataset_type=\"train\", columns=['source', 'type', 'year', 'id', 'score', 'sent_a', 'sent_b'],preprocessing=False,verbose=True):\n",
        "  path = INPUT_PATH + \"sts-\"+ dataset_type + \".csv\"\n",
        "  \"\"\"\n",
        "  Take the input path and return the dataframe\n",
        "  \"\"\"\n",
        "  if verbose == True:\n",
        "    print(path)\n",
        "\n",
        "  df=pd.read_csv(path,sep='\\t',quoting=csv.QUOTE_NONE,names=columns)\n",
        "  if preprocessing==True:\n",
        "    df['sent_a'] = df['sent_a'].apply(preprocess)\n",
        "    df['sent_b'] = df['sent_b'].apply(preprocess)\n",
        "  return df\n",
        "\n",
        "df_train = read_sts_csv(\"train\",preprocessing=False)\n",
        "df_dev = read_sts_csv(\"dev\",preprocessing=False)\n",
        "df_test = read_sts_csv(\"test\",preprocessing=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gO2ZkIwDmo4s",
      "metadata": {
        "id": "gO2ZkIwDmo4s"
      },
      "source": [
        "## Hyperparameters: 5 Marks\n",
        "Update this cell with you choosen parameters except, NUM_EPOCHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "id": "4QurhOG7E0Z-",
      "metadata": {
        "id": "4QurhOG7E0Z-"
      },
      "outputs": [],
      "source": [
        "NON_CONEXTUAL_MODEL_TYPE = \"glove-wiki-gigaword-50\"\n",
        "CONEXTUAL_MODEL_TYPE = \"sentence-transformers/stsb-distilbert-base\"\n",
        "HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL = \"\" # USE THE HUGGAING FACE VERSION OF SENTENCE_TRANSFORMER_TYPE\n",
        "INPUT_PATH = \"content/\"\n",
        "\n",
        "BATCH_SIZE = \"\"\n",
        "OUT_DIM_DENSE = \"\"\n",
        "NUM_EPOCHS = 2 ## THIS IS FIXED DO NOT CHANGE\n",
        "\n",
        "# You are free to add your own hyperparameters as well."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KgpbPlH9nXDy",
      "metadata": {
        "id": "KgpbPlH9nXDy"
      },
      "source": [
        "## CONFIGURATION 1: Non-contextual Embeddings + ML Regression: 8 marks\n",
        "1 Load the non-contextual embedding model in variable `non_cont_model1`. **1 marks**\n",
        "\n",
        "2 Get feature for the sentences using the LM model loaded before. Add the code in the `get_feature_model1()` **2 marks**\n",
        "\n",
        "2 Using features as X and score as Y, train a ML based regression model (`model1`). You are free to choose any sklearn based regression method, and its hyperparameters. **3.5 marks**\n",
        "\n",
        "3 Print the correlation scores on the dev and test set predictions using trained `model1`. **1.5 mark**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "id": "cf17ce5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "glove = gensim.downloader.load(NON_CONEXTUAL_MODEL_TYPE)\n",
        "\n",
        "def get_sentence_vector(sentence):\n",
        "    sentence_vector = []\n",
        "    for word in sentence.split():\n",
        "        if word in glove:\n",
        "            sentence_vector.append(glove[word])\n",
        "    return np.mean(sentence_vector, axis=0)\n",
        "\n",
        "\n",
        "def get_sentence_vectors(sentences):\n",
        "    return np.array([get_sentence_vector(sentence) for sentence in sentences])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "id": "Hr7teQO9nfRR",
      "metadata": {
        "id": "Hr7teQO9nfRR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAINING :\n",
            "      Iter       Train Loss   Remaining Time \n",
            "         1           2.1139           13.83s\n",
            "         2           2.0857           14.47s\n",
            "         3           2.0590           13.21s\n",
            "         4           2.0363           12.56s\n",
            "         5           2.0069           12.15s\n",
            "         6           1.9891           11.83s\n",
            "         7           1.9689           11.53s\n",
            "         8           1.9455           11.28s\n",
            "         9           1.9282           11.07s\n",
            "        10           1.9089           10.93s\n",
            "        20           1.7569            9.67s\n",
            "        30           1.6532            8.55s\n",
            "        40           1.5750            7.41s\n",
            "        50           1.5004            6.18s\n",
            "        60           1.4485            4.94s\n",
            "        70           1.3838            3.71s\n",
            "        80           1.3436            2.47s\n",
            "        90           1.2942            1.23s\n",
            "       100           1.2597            0.00s\n",
            "\n",
            "EVALUATION :\n",
            "Spearman Correlation on Dev Set :  0.43416547183416593\n",
            "Spearman Correlation on Test Set :  0.41784946921582544\n"
          ]
        }
      ],
      "source": [
        "# non_cont_model1 \n",
        "non_cont_model1 = get_sentence_vectors\n",
        "\n",
        "def get_feature_model1(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the embedding vectors for the each sentence column using non_cont_model1,\n",
        "  Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "  \"\"\"\n",
        "  return non_cont_model1(data_frame['sent_a']), non_cont_model1(data_frame['sent_b'])\n",
        "\n",
        "\n",
        "df_train = read_sts_csv(\"train\",preprocessing=True,verbose=False)\n",
        "df_dev = read_sts_csv(\"dev\",preprocessing=True,verbose=False)\n",
        "df_test = read_sts_csv(\"test\",preprocessing=True,verbose=False)\n",
        "\n",
        "# feature_1_<dataset_type>, feature_2_<dataset_type> = get_feature_model2(data_frame)\n",
        "feature_1_train, feature_2_train = get_feature_model1(df_train)\n",
        "feature_1_dev, feature_2_dev = get_feature_model1(df_dev)\n",
        "feature_1_test, feature_2_test = get_feature_model1(df_test)\n",
        "\n",
        "X_train = np.concatenate((feature_1_train, feature_2_train), axis=1)\n",
        "X_dev = np.concatenate((feature_1_dev, feature_2_dev), axis=1)\n",
        "X_test = np.concatenate((feature_1_test, feature_2_test), axis=1)\n",
        "\n",
        "\n",
        "y_train = df_train['score'].values\n",
        "y_dev = df_dev['score'].values\n",
        "y_test = df_test['score'].values\n",
        "\n",
        "\n",
        "\n",
        "# Initiate a regression model and train it.\n",
        "print(\"TRAINING :\")\n",
        "reg = GradientBoostingRegressor(random_state=0,verbose=True)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Print spearmanr correlation on the predicted output of the dev and test sets.\n",
        "print(\"\\nEVALUATION :\")\n",
        "print(\"Spearman Correlation on Dev Set : \",stats.spearmanr(y_dev, reg.predict(X_dev)).correlation)\n",
        "print(\"Spearman Correlation on Test Set : \",stats.spearmanr(y_test, reg.predict(X_test)).correlation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DBzjbQ-grL8H",
      "metadata": {
        "id": "DBzjbQ-grL8H"
      },
      "source": [
        "## CONFIGURATION 2: Contextual Embeddings + ML Regression: 7 marks\n",
        "1 Load the contextual embedding model in variable `non_cont_model2`. **1 marks**\n",
        "\n",
        "2 Get feature for the sentences using the LM model loaded before. Add the code in the `get_feature_model2()` **2 marks**\n",
        "\n",
        "2 Using features as X and score as Y, train a ML based regression model (`model2`). You are free to choose any sklearn based regression method, and its hyperparameters. **3.5 marks**\n",
        "\n",
        "3 Print the correlation scores on the dev and test set predictions using trained `model2`. **1.5 mark**\n",
        "\n",
        "Useful references: https://www.sbert.net/docs/usage/semantic_textual_similarity.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "id": "GlTVNjv0sNP0",
      "metadata": {
        "id": "GlTVNjv0sNP0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAINING :\n",
            "      Iter       Train Loss   Remaining Time \n",
            "         1           2.1042            3.00m\n",
            "         2           2.0682            2.99m\n",
            "         3           2.0262            2.96m\n",
            "         4           1.9845            2.93m\n",
            "         5           1.9397            2.94m\n",
            "         6           1.8976            2.91m\n",
            "         7           1.8495            2.88m\n",
            "         8           1.8155            2.84m\n",
            "         9           1.7879            2.81m\n",
            "        10           1.7494            2.78m\n",
            "        20           1.4535            2.50m\n",
            "        30           1.2247            2.16m\n",
            "        40           1.0729            1.85m\n",
            "        50           0.9364            1.54m\n",
            "        60           0.8515            1.24m\n",
            "        70           0.7770           55.81s\n",
            "        80           0.7146           37.28s\n",
            "        90           0.6606           18.66s\n",
            "       100           0.6025            0.00s\n",
            "\n",
            "EVALUATION :\n",
            "Spearman Correlation on Dev Set :  0.7321888818647269\n",
            "Spearman Correlation on Test Set :  0.7207345747750844\n"
          ]
        }
      ],
      "source": [
        "# non_cont_model2\n",
        "non_cont_model2 = SentenceTransformer(CONEXTUAL_MODEL_TYPE)\n",
        "\n",
        "def get_feature_model2(df):\n",
        "    \"\"\"\n",
        "    Input a data frame and return the embedding vectors for the each sentence column using non_cont_model2,\n",
        "    Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "    \"\"\"\n",
        "    return non_cont_model2.encode(df['sent_a']), non_cont_model2.encode(df['sent_b'])\n",
        "\n",
        "\n",
        "df_train = read_sts_csv(\"train\",preprocessing=True,verbose=False)\n",
        "df_dev = read_sts_csv(\"dev\",preprocessing=True,verbose=False)\n",
        "df_test = read_sts_csv(\"test\",preprocessing=True,verbose=False)\n",
        "\n",
        "\n",
        "# feature_1_<dataset_type>, feature_2_<dataset_type> = get_feature_model2(data_frame)\n",
        "feature_1_train, feature_2_train = get_feature_model2(df_train)\n",
        "feature_1_dev, feature_2_dev = get_feature_model2(df_dev)\n",
        "feature_1_test, feature_2_test = get_feature_model2(df_test)\n",
        "\n",
        "\n",
        "# X_<dataset_type>, Y_<dataset_type> = \n",
        "X_train = np.concatenate((feature_1_train, feature_2_train), axis=1)\n",
        "X_dev = np.concatenate((feature_1_dev, feature_2_dev), axis=1)\n",
        "X_test = np.concatenate((feature_1_test, feature_2_test), axis=1)\n",
        "\n",
        "\n",
        "y_train = df_train['score'].values\n",
        "y_dev = df_dev['score'].values\n",
        "y_test = df_test['score'].values\n",
        "\n",
        "\n",
        "# Initiate a regression model and train it.\n",
        "print(\"TRAINING :\")\n",
        "reg2 = GradientBoostingRegressor(random_state=0,verbose=True)\n",
        "reg2.fit(X_train, y_train)\n",
        "\n",
        "# Print spearmanr correlation on the predicted output of the dev and test sets.\n",
        "print(\"\\nEVALUATION :\")\n",
        "print(\"Spearman Correlation on Dev Set : \",stats.spearmanr(y_dev, reg2.predict(X_dev)).correlation)\n",
        "print(\"Spearman Correlation on Test Set : \",stats.spearmanr(y_test, reg2.predict(X_test)).correlation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35135bc9",
      "metadata": {},
      "source": [
        "### SAVING ML BASED REGRESSOR MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4667241c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Save the model\n",
        "with open('models/reg.pkl', 'wb') as f:\n",
        "    pickle.dump(reg, f)\n",
        "\n",
        "with open('models/reg2.pkl', 'wb') as f:\n",
        "    pickle.dump(reg2, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VImljTWps_GR",
      "metadata": {
        "id": "VImljTWps_GR"
      },
      "source": [
        "## CONFIGURATION 3: Fine-Tune a Contextual Embeddings Model: 18 marks\n",
        "1 Prepare data samples to be for the DL model to consume. Add the code in the `form_data()`. **4 marks**\n",
        "\n",
        "3 Create the data loader, one each for train/dev/test data_input sample set obtained from `form_input_example()`. **1.5 marks**\n",
        "\n",
        "4 Initiate `model3` consisting of **atleast** the following 3 components - `base_LM`, a `pooling_layer` and a `dense_layer`. Use appropriate activation function in dense. **Atleast** one layer of `base_LM` should be set to trainable. **5 marks**\n",
        "\n",
        "6 Initiate the `loss`. **0.5 marks**\n",
        "\n",
        "7 Fit the `model3`. Use `NUM_EPOCHS = 2`. **MAX_NUM_EPOCHS allowed will be 3**. **2 marks** \n",
        "\n",
        "8 Complete the `get_model_predicts()` to obtain predicted scores for input sentence pairs. **3.5 marks** \n",
        "\n",
        "9 Print the correlation scores on the dev and test set predictions. **1.5 mark**\n",
        "\n",
        "Useful References: https://huggingface.co/blog/how-to-train-sentence-transformers "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "id": "0kb0xJZmZGIR",
      "metadata": {
        "id": "0kb0xJZmZGIR"
      },
      "outputs": [],
      "source": [
        "def form_data(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the dataloder.\n",
        "  \"\"\"\n",
        "\n",
        "def get_model_predicts(data_type, trained_model):\n",
        "  \"\"\"\n",
        "  Input the dataset list and return a list of cosine similarity scores. Use the fitted final_trainable_model for obtaining encodings.\n",
        "  \"\"\"\n",
        "\n",
        "# dataloader_<dataset_type> = form_data(data_frame)\n",
        "# base_model = \n",
        "# layer_ppoling = \n",
        "# layer_dense = \n",
        "# model3 = \n",
        "# loss =\n",
        "\n",
        "# Fit the model3.\n",
        "# Print spearman correlation on the predicted output of the dev and test sets."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
